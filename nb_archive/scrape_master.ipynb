{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everyday nwac.us gives a forecast for each area across the state, giving each area it's own unique url. The urls are nested within dates as well, so this notebook will perform a scrape to generate a list of urls within each day.\n",
    "\n",
    "### These daily urls also contain more list of url forecasts. Once saved, a mongodb is where the htmls will be collected, which will then be utilized for the next scrape to then take the html code using beautifulsoup!\n",
    "\n",
    "\n",
    "## import scrape_soup with the function and other module imports\n",
    "\n",
    "## do not initiate another shadow nor close the first one that opens when the module loads, for the functions rely on the original window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import Chrome\n",
    "import pymongo\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import scrape_soup as s_soup\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a list of years to scrape, then pass the list through the collect_day_urls function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2014,2020)\n",
    "browser = Chrome()\n",
    "daily_master_list = s_soup.collect_day_urls(years,browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1514"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(daily_master_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes quite a while, rather than running the cell below, \n",
    "#just grab the file 'master_url_list.json'\n",
    "\n",
    "browser = Chrome()\n",
    "s_soup.get_mountain_urls_from_day_urls(daily_master_list,browser,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = pd.read_json('master_url_list.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rather than parsing through all webpages, I will specify just the stevens pass urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stevens = urls_df.loc[urls_df['url'].str.contains('cascade-west-stevens-pass')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stevens_urls = list(stevens['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stevens_urls also has forcasts for north of stevens-pass, lets try and filter that out back in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stevens_true = stevens.loc[~stevens[\"url\"].str.contains(\"north\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stevens_list = list(stevens_true['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stevens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = pymongo.MongoClient()\n",
    "db = mc['avalanche']\n",
    "forecast_coll = db['stevens_forecasts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_url(url, browser=browser, delay=3):\n",
    "#     \"\"\"Returns the HTML source from a URL.\"\"\"\n",
    "#     browser.get(url)\n",
    "#     time.sleep(delay)\n",
    "#     html = browser.page_source\n",
    "#     return html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_page(url, browser=browser, coll=coll, delay=3):\n",
    "#     \"\"\"Scrapes and saves the source of a web page.\"\"\"\n",
    "#     docs = list(coll.find({'url': url}))\n",
    "#     if len(docs) == 0:\n",
    "#         html = scrape_url(browser=browser,\n",
    "#                           url=url,\n",
    "#                           delay=delay)\n",
    "#         coll.insert_one({\n",
    "#             'url': url,\n",
    "#             'html': html,\n",
    "#         })\n",
    "#     doc = coll.find_one({'url' : url})\n",
    "#     return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in stevens_list:\n",
    "    s_soup.collect_page(url,browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forecasts_test', 'stevens_forecasts', 'forecasts']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = coll.find({})\n",
    "stevens = [forecasts for forecasts in cur ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stevens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have all the desired html pages, beautiful soup can be implemented to grab the forecasts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_danger_info(page):\n",
    "#     '''Collects the relevant info from the webpage html'''\n",
    "    \n",
    "    \n",
    "#     danger_above_tag = '#treeline-above > div.span4.elev-day1-column > div.danger-description > h4'\n",
    "#     danger_near_tag ='#treeline-near > div.span4.elev-day1-column > div.danger-description > h4'\n",
    "#     danger_below_tag = '#treeline-below > div.span4.elev-day1-column > div.danger-description > h4'\n",
    "#     area_tag = '#main-content > h2'\n",
    "#     date_tomorrow_tag = '#elevation-levels-header > div.span4.desktop.elevation-day-name > p'\n",
    "#     soup = BeautifulSoup(page['html'],'lxml')\n",
    "#     area = soup.select_one(area_tag).text\n",
    "#     date_tomorrow = soup.select_one(date_tomorrow_tag).text\n",
    "#     danger_above = soup.select_one(danger_above_tag).text\n",
    "#     danger_near = soup.select_one(danger_near_tag).text\n",
    "#     danger_below = soup.select_one(danger_below_tag).text\n",
    "#     return {\n",
    "#             'date_tomorrow': date_tomorrow,\n",
    "#             'area': area,\n",
    "#             'danger_above_treeline': danger_above,\n",
    "#             'danger_near_treeline': danger_near,\n",
    "#             'danger_below_treeline': danger_below,\n",
    "#             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stevens_forecast_danger_test.json','w') as f:\n",
    "    for page in stevens:\n",
    "        try:\n",
    "            line = s_soup.create_danger_info(page)\n",
    "            json.dump(line, f)\n",
    "            f.write('\\n')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('json_archive/stevens_forecast_danger.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up data for nulls and duplicates before finalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((967, 5), (967, 5), (830, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df.dropna().shape, df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "area                     0\n",
       "danger_above_treeline    0\n",
       "danger_below_treeline    0\n",
       "danger_near_treeline     0\n",
       "date_tomorrow            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('stevens_forecast_soup.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
